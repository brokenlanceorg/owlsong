<html>

	<head>
		<title>
		The Abstract Shire : Dedication to Logic : Mathematics : Physics
		</title>

	<link rel=StyleSheet href="../Declarations.css" type="text/css">

	</head>

<body>

	<body bgcolor=black>

      <center><img src="../Physics.jpg"></center>
      <img src="../graybar2.gif" width=100% height=9px>

      <center>
      <table cellpadding=20 cellspacing=10 width=90%>
      <tr> 
      <td bgcolor=#222222 valign=top width=50%>
      <div class=bodyWhiteSmallJustify>
         <p>
The neuron’s sigmoid function then operates on the proper quantities (III.1) or (III.2) to compute 
the neuron’s output impulse, which can be denoted by:
         </p>
<center>
h<sub>j</sub> = <span class="symbol1"><span class="symbol2">s</span></span>
(<span class="symbol1"><span class="symbol2">h</span></span><sub>j</sub>) &nbsp &nbsp &nbsp &nbsp &nbsp (III.3)
</center>
<center>
o<sub>k</sub> = <span class="symbol1"><span class="symbol2">s</span></span>
(<span class="symbol1"><span class="symbol2">w</span></span><sub>k</sub>) &nbsp &nbsp &nbsp &nbsp &nbsp (III.4)
</center>
         <p>
Observe that we may write the output from a given neuron network on a particular input data instance as:
         </p>
<center>
o<sub>k</sub> = <span class="symbol1"><span class="symbol2">s</span></span>
(<span class="symbol1"><span class="symbol2">w</span></span><sub>k</sub>) = 
<span class="symbol1"><span class="symbol2">s</span></span>
(<span class="symbol1"><span class="symbol2">s</span></span><sub>j</sub>(x
<span class="symbol1"><span class="symbol2">&#215</span></span> W<sub>j</sub>)
<span class="symbol1"><span class="symbol2">&#215</span></span> W<sub>k</sub>)
&nbsp &nbsp &nbsp &nbsp &nbsp (III.5)
</center>
         <p>
Interestingly, it is evident that the right-hand side of (III.5) is of the form:
         </p>
<center>
<span class="symbol1"><span class="symbol2">&#166</span></span>(x<sub>1</sub>, x<sub>2</sub>,...,x<sub>n</sub>)=
<span class="symbol1"><span class="symbol2">s</span></span>
[<span class="symbol1"><span class="symbol2">&#229</span></span><span class="symbol1"><span class="symbol2">s</span></span><sub>j</sub>
(<span class="symbol1"><span class="symbol2">&#229</span></span> x<sub>i</sub><span class="symbol1"><span class="symbol2">&#215</span></span> W<sub>j</sub>
)<span class="symbol1"><span class="symbol2">&#215</span></span> W<sub>k</sub>]
&nbsp (III.6)
</center>
         <p>
where the outer sum is over the j index and the inner sum over the i index. Evidently, since the neural network maps E<sup>n</sup> into E<sup>n</sup>, this 
equation is a general form of Sprecher’s result, equation (II.20), which contains (II.2) as a special case, thus Hilbert’s thirteenth 
problem can be solved by the computational structure of the backward error propagation neural network.
         </p>
      </div>
      </td>

	<td bgcolor=#222222 valign=top width=50%>
      <div class=bodyWhiteSmallJustify>
         <p>
            <span class=bodyGreenSmallJustify>[IV Derivation of the Backpropagation Algorithm]</span>
         </p>
         <p>
In order to visualize the problem at hand, it will be advantageous to consider the situation geometrically. First, we denote the training data 
set by: {(x<sup>p</sup><sub>i</sub>, T<sup>p</sup><sub>k</sub>)} for 1 
<span class="symbol1"><span class="symbol2">&#163</span></span> p 
<span class="symbol1"><span class="symbol2">&#163</span></span> P, x<sup>p</sup><sub>i</sub> 
<span class="symbol1"><span class="symbol2">&#206</span></span> <span class="symbol1"><span class="symbol2">&#194</span></span>, 
and T<sup>p</sup><sub>k</sub> <span class="symbol1"><span class="symbol2">&#206</span></span> <span class="symbol1"><span class="symbol2">&#194</span></span>, 
where P is the number of training instances. Thus, x<sup>p</sup><sub>i</sub> is the input instance and T<sup>p</sup><sub>k</sub> is 
the associated output for which we want the network to approximate. 
         </p>
         <p>
Consider now the quantity 
<span class="symbol1"><span class="symbol2">d</span></span><sup>p</sup><sub>k</sub> 
= (o<sup>p</sup><sub>k</sub> - T<sup>p</sup><sub>k</sub>) so that 
<span class="symbol1"><span class="symbol2">d</span></span><sup>p</sup><sub>k</sub>
 is a vector of the output layer’s difference from the actual target output. We shall, in the following derivation, 
dispense with the p notation, since our analysis is concerned with a particular training instance p, but the generalization for all training 
instances P will be easily made shortly.
         </p>
         <p>
Let E<sub>0</sub> be defined as: &#189<span class="symbol1"><span class="symbol2">&#229</span></span>(<span class="symbol1"><span class="symbol2">d</span></span><sub>k</sub>)<sup>2</sup>, 
where the summation is over the index k, (the number of output neurons).Then, E<sub>0</sub> defines a surface 
in weight space W<sub>ijk</sub> or dendrite-axon space however one prefers to imagine it. It is evident that this quantity, the overall error in 
the approximation, must be minimized.
         </p>
       </div>
       </td>
       </tr>
       </table>
       </center>


      <center>
	<table cellpadding=0 cellspacing=10 width=100%>
      <tr>
      <td>
         <div align=left>
         <a href="BWNeural10.htm" target="Main"><img src="../previous.gif" border=no></a>
         </div>
      </td>
      <td> 
      <center>
         <span class=linkTextWhite>[</span>
         <a href="../BWMain.htm" target="Main"><span class=linkTextWhite>Home</span></a>
         <span class=linkTextWhite>|</span>
         <a href="../BWMathematics.htm" target="Main"><span class=linkTextWhite>Mathematics</span></a>
         <span class=linkTextWhite>|</span>
         <a href="../BWPhysics.htm" target="Main"><span class=linkTextWhite>Physics</span></a>
         <span class=linkTextWhite>|</span>
         <a href="../BWPhilosophy.htm" target="Main"><span class=linkTextWhite>Philosophy</span></a>
         <span class=linkTextWhite>]</span>
      </center>
      </td>

      <td>
         <div align=right>
         <a href="BWNeural12.htm" target="Main"><img src="../next.gif" border=no></a>
         </div>
      </td>

      </tr>

      <tr>
      <td>
      </td>
      <td>
      <span class=footnoteRedSmall>
         <center>
         Design and Content by Brandon Benham<br>
         <a href="mailto:brandbn@attglobal.net"><img src="../mailto4.gif" border=no></a>
         </center>
      </span>
      </td>
      </tr>
      </table>
      </center>

</body>


</html>
<!--             <span class="symbol1"><span class="symbol2">m</span></span> -->